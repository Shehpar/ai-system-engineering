# AI Infrastructure Anomaly Detection System

**Status**: âœ… Production Ready

---

## ğŸ¯ Overview

A production-grade AI system for **real-time infrastructure anomaly detection** using Isolation Forest machine learning. Detects anomalies in system metrics (CPU, Memory, Network) with sub-10ms latency.

### Key Capabilities

- **ML Pipeline**: Offline training with grid search optimization, real-time inference
- **Explainability**: SHAP values for per-feature anomaly contributions
- **Monitoring**: Grafana dashboards + InfluxDB time-series database
- **Experiment Tracking**: MLflow for reproducibility and model management
- **Data Ingestion**: Multiple sources (Flask simulation, Telegraf real metrics)
- **Stress Testing**: HTTP load generation for system validation
- **Containerization**: Docker Compose orchestration for all services
- **Testing**: Unit tests with pytest, CI/CD via GitHub Actions

---

## ğŸ—ï¸ System Architecture

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRESENTATION LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Grafana Dashboard (Port 3000)                                  â”‚
â”‚  â”œâ”€ Real-time anomaly visualization                            â”‚
â”‚  â”œâ”€ System metrics charts                                       â”‚
â”‚  â””â”€ SHAP feature contribution panels                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EXPERIMENT & ANALYTICS LAYER                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MLflow UI (Port 5000)                                          â”‚
â”‚  â”œâ”€ Experiment tracking                                         â”‚
â”‚  â”œâ”€ Model artifacts storage                                     â”‚
â”‚  â””â”€ Run history & parameters                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INFERENCE & ML LAYER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Python ML Pipeline                                             â”‚
â”‚  â”œâ”€ detect_anomaly.py â†’ Real-time predictions + SHAP values   â”‚
â”‚  â”œâ”€ train_model.py â†’ Offline model training (grid search)      â”‚
â”‚  â”œâ”€ validate_data.py â†’ Data quality checks (6 validations)     â”‚
â”‚  â””â”€ evaluate_model.py â†’ Robustness testing (4 scenarios)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATA STORAGE & TIME-SERIES LAYER                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  InfluxDB (Port 8086)                                           â”‚
â”‚  â”œâ”€ Measurement: system_metrics (CPU, Memory, Network)          â”‚
â”‚  â”œâ”€ Measurement: ai_predictions (anomaly scores, SHAP values)   â”‚
â”‚  â””â”€ Retention: Time-series data for trend analysis              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATA COLLECTION & INGESTION LAYER               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Source 1: Flask App (Simulated Metrics)                        â”‚
â”‚  â”œâ”€ generate_metrics() â†’ CPU, Memory, Network                   â”‚
â”‚  â””â”€ HTTP stress testing endpoint                                â”‚
â”‚                                                                 â”‚
â”‚  Source 2: Telegraf (Real Metrics - via datacenter/)            â”‚
â”‚  â”œâ”€ System resource collection                                  â”‚
â”‚  â””â”€ Pushes to InfluxDB                                          â”‚
â”‚                                                                 â”‚
â”‚  Source 3: Real Data Collector                                  â”‚
â”‚  â””â”€ collect_real_data.py â†’ Extracts 24h historical data        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interactions

```
Setup Phase:
1. Docker Compose starts: ai_app â†’ influxdb, grafana, mlflow
2. ai_app container:
   - Installs Python deps from requirements.txt
   - Prepares /app directory
   - Awaits container health checks

Training Phase:
3. train_model.py:
   - Loads data/processed/system_metrics_processed.csv
   - Grid search: contamination âˆˆ {0.01, 0.05, 0.1}
   - Selects best Isolation Forest model
   - Saves to models/anomaly_model_*.pkl
   - Logs metrics to MLflow

Inference Phase:
4. detect_anomaly.py (runs continuously):
   - Reads latest metrics from InfluxDB
   - Predicts anomalies using trained model
   - Computes SHAP values per prediction
   - Writes predictions + SHAP to InfluxDB (ai_predictions)
   - Grafana consumes and visualizes in real-time

Validation Phase:
5. validate_data.py:
   - Schema checks (required columns)
   - Range validations (0-100 for CPU/mem, â‰¥0 for network)
   - Missing data, duplicates, outliers
   - IQR-based anomaly detection

Evaluation Phase:
6. evaluate_model.py:
   - Robustness scenarios: noise, missing data, outliers, shifts
   - Produces results/evaluation_report_*.json
```

### Technology Stack

| Layer | Technology | Version | Port | Purpose |
|-------|-----------|---------|------|---------|
| **Runtime** | Python | 3.11, 3.12 | â€” | ML model execution |
| **Container** | Docker | 29.1.3+ | â€” | Service isolation |
| **Orchestration** | Docker Compose | 2.0+ | â€” | Multi-service coordination |
| **Database** | InfluxDB | 1.8.10 | 8086 | Time-series metrics storage |
| **Visualization** | Grafana | latest | 3000 | Dashboard & alerting |
| **ML Experiment** | MLflow | 2.0+ | 5000 | Tracking & model registry |
| **ML Framework** | scikit-learn | 1.8.0 | â€” | Isolation Forest algorithm |
| **Explainability** | SHAP | 0.42.0+ | â€” | Feature contribution analysis |

---

## ğŸ“‹ Installation

### Docker Deployment (Recommended)

```bash
# Clone and enter directory
git clone https://github.com/Shehpar/ai-system-engineering.git
cd ai-infrastructure-anomaly-detection

# Start all services
docker-compose -f docker/docker-compose.yml up -d

# Verify healthy status
docker-compose -f docker/docker-compose.yml ps

# View logs
docker-compose -f docker/docker-compose.yml logs -f ai_app

# Stop services
docker-compose -f docker/docker-compose.yml down
```

### Local Python Setup

```bash
# Requires Python 3.11+ (CI runs on 3.11 and 3.12)
# Install dependencies
pip install -r requirements.txt

# Train model
python src/train_model.py

# Validate data
python src/validate_data.py

# Evaluate robustness
python src/evaluate_model.py

# Run tests
pytest tests/ -v
```

---

## ğŸ§ª Testing

### Run Unit Tests
```bash
# Via Docker
docker-compose -f docker/docker-compose.yml exec -T ai_app pytest tests/ -v

# Locally
pytest tests/ -v
```

**Test Results**:
- test_split_data_shapes: âœ… PASSED
- test_validate_schema_pass: âœ… PASSED
- test_validate_schema_fail: âœ… PASSED
- test_validate_ranges_fail_when_outside_bounds: âœ… PASSED
- test_validate_live_data: âœ… PASSED

**Total**: 5/5 tests passing (100%)

---

## ğŸ“Š Model Performance (Jan 28, 2026)

### Baseline Metrics
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| **Precision** | 100.0% | â‰¥90% | âœ… EXCEEDS |
| **Recall** | 72.73% | â‰¥90% | âš ï¸ ACCEPTABLE |
| **F1-Score** | 84.21% | â‰¥85% | âš ï¸ NEAR |
| **ROC-AUC** | 100.0% | â‰¥95% | âœ… EXCEEDS |
| **Latency** | 6.94ms | <5s | âœ… EXCEEDS |

### Robustness Results
- âœ… **Noise**: Stable under Ïƒ=0.01-0.1 Gaussian noise
- âœ… **Missing Data**: Handles feature imputation gracefully
- âœ… **Outliers**: 100% detection rate for 10x magnitude anomalies
- âœ… **SHAP**: Explains per-feature contributions to anomalies

See [docs/MODEL_CARD.md](docs/MODEL_CARD.md) for detailed results.

---

## ğŸ“ Project Structure & Directory Architecture

### Core Directory Layout

```
ai-infrastructure-anomaly-detection/                         â† Root project folder
â”‚
â”œâ”€â”€ ğŸ src/                                                   â† ML Pipeline (Python scripts)
â”‚   â”œâ”€â”€ train_model.py                                        # Offline model training
â”‚   â”‚   â”œâ”€ Loads: data/processed/system_metrics_processed.csv
â”‚   â”‚   â”œâ”€ Algorithm: Isolation Forest with grid search
â”‚   â”‚   â”œâ”€ Grid: contamination âˆˆ {0.01, 0.05, 0.1}
â”‚   â”‚   â”œâ”€ Output: models/anomaly_model_*.pkl
â”‚   â”‚   â””â”€ Tracking: Logs to MLflow (metrics, params, artifacts)
â”‚   â”‚
â”‚   â”œâ”€â”€ validate_data.py                                      # Data Quality Checks
â”‚   â”‚   â”œâ”€ Validates: Required columns, types, ranges
â”‚   â”‚   â”œâ”€ Anomaly Detection: IQR-based outlier detection
â”‚   â”‚   â”œâ”€ Missing Data: Identifies gaps & duplicates
â”‚   â”‚   â””â”€ Output: Validation reports + cleaned data
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluate_model.py                                     # Robustness Testing
â”‚   â”‚   â”œâ”€ Scenario 1: Gaussian noise injection (Ïƒ=0.01-0.1)
â”‚   â”‚   â”œâ”€ Scenario 2: Missing value imputation
â”‚   â”‚   â”œâ”€ Scenario 3: Extreme outlier detection (10x magnitude)
â”‚   â”‚   â”œâ”€ Scenario 4: Data shift simulation
â”‚   â”‚   â””â”€ Output: results/evaluation_report_*.json
â”‚   â”‚
â”‚   â”œâ”€â”€ detect_anomaly.py                                     # Real-time Inference [SHAP]
â”‚   â”‚   â”œâ”€ Loads: Trained model from models/
â”‚   â”‚   â”œâ”€ Input: Real-time metrics from InfluxDB (system_metrics)
â”‚   â”‚   â”œâ”€ Process: Prediction + SHAP explainability analysis
â”‚   â”‚   â”œâ”€ Output: Writes to InfluxDB (ai_predictions measurement)
â”‚   â”‚   â”‚   â”œâ”€ Fields: anomaly_score, is_anomaly, latency_ms
â”‚   â”‚   â”‚   â””â”€ SHAP fields: shap_cpu, shap_memory, shap_network
â”‚   â”‚   â””â”€ Feature: Per-feature contribution explanation
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing.py                                      # Data Transformations
â”‚   â”‚   â”œâ”€ Standardization: Mean=0, Std=1
â”‚   â”‚   â”œâ”€ Handling: Missing values, outliers
â”‚   â”‚   â””â”€ Pipeline: Preparation for training/inference
â”‚   â”‚
â”‚   â”œâ”€â”€ data_generation.py                                    # Synthetic Data Creation
â”‚   â”‚   â”œâ”€ Generates: Normal distribution metrics
â”‚   â”‚   â”œâ”€ Injects: Anomalies (spikes, trends)
â”‚   â”‚   â””â”€ Output: CSV files for testing
â”‚   â”‚
â”‚   â””â”€â”€ collect_real_data.py                                  # Real Data Extraction
â”‚       â”œâ”€ Pulls: 24-hour historical data from InfluxDB
â”‚       â”œâ”€ Aggregation: Time-bucket averaging
â”‚       â””â”€ Output: data/raw/system_metrics.csv
â”‚
â”œâ”€â”€ ğŸ§ª tests/                                                 â† Unit Tests (pytest)
â”‚   â”œâ”€â”€ conftest.py                                           # pytest configuration & fixtures
â”‚   â”œâ”€â”€ test_train_model.py                                   # Training pipeline tests
â”‚   â”‚   â”œâ”€ Model creation & hyperparameter sensitivity
â”‚   â”‚   â”œâ”€ Grid search correctness
â”‚   â”‚   â””â”€ Artifact generation validation
â”‚   â”‚
â”‚   â””â”€â”€ test_validate_data.py                                 # Data validation tests
â”‚       â”œâ”€ Schema & type validation
â”‚       â”œâ”€ Range constraints
â”‚       â””â”€ Anomaly detection accuracy
â”‚
â”œâ”€â”€ ğŸ³ docker/                                                â† Containerization
â”‚   â”œâ”€â”€ Dockerfile                                            # Python 3.11-slim base
â”‚   â”‚   â”œâ”€ Base: python:3.11-slim (minimal, ~170MB)
â”‚   â”‚   â”œâ”€ Dependencies: Installs from requirements.txt
â”‚   â”‚   â”œâ”€ Entrypoint: Runs Python scripts via docker-compose
â”‚   â”‚   â””â”€ Health Check: Container health status monitoring
â”‚   â”‚
â”‚   â””â”€â”€ docker-compose.yml                                    # Service Orchestration (4 services)
â”‚       â”œâ”€ Service 1: ai_app (Custom Python container)
â”‚       â”‚   â”œâ”€ Container: anomaly-detection-app
â”‚       â”‚   â”œâ”€ Volumes: Maps src/, data/, models/, results/
â”‚       â”‚   â”œâ”€ Env: Python ML pipeline environment
â”‚       â”‚   â””â”€ Dependencies: Depends on influxdb health
â”‚       â”‚
â”‚       â”œâ”€ Service 2: influxdb:1.8.10
â”‚       â”‚   â”œâ”€ Database: system_metrics (raw) + ai_predictions (inferences)
â”‚       â”‚   â”œâ”€ Port: 8086 (HTTP API)
â”‚       â”‚   â”œâ”€ Volumes: influxdb-storage (persists data)
â”‚       â”‚   â””â”€ Health Check: Queries /_health endpoint
â”‚       â”‚
â”‚       â”œâ”€ Service 3: grafana:latest
â”‚       â”‚   â”œâ”€ UI: http://localhost:3000 (admin/admin)
â”‚       â”‚   â”œâ”€ Provisioning: Auto-configures InfluxDB datasource
â”‚       â”‚   â”œâ”€ Dashboards: Auto-loads from grafana/dashboards/
â”‚       â”‚   â””â”€ Features: Anomaly viz, SHAP panels, alerting
â”‚       â”‚
â”‚       â””â”€ Service 4: mlflow (Optional, for experiment tracking)
â”‚           â”œâ”€ UI: http://localhost:5000
â”‚           â”œâ”€ Backend: SQLite (mlflow/mlflow.db)
â”‚           â””â”€ Storage: Artifacts in mlflow/artifacts/
â”‚
â”œâ”€â”€ ğŸ“Š grafana/                                               â† Dashboard Configuration
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ influxdb.yml                                  # Auto-provisioned InfluxDB connector
â”‚   â”‚   â”‚       â”œâ”€ Host: influxdb:8086
â”‚   â”‚   â”‚       â”œâ”€ Database: anomaly_detection
â”‚   â”‚   â”‚       â””â”€ Is default: true
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ dashboard.yml                                 # Dashboard provider config
â”‚   â”‚       â””â”€â”€ AI Anomaly Detection - Advanced...json        # Pre-built dashboard JSON
â”‚   â”‚           â”œâ”€ Panels: Metrics time series, anomaly heatmap
â”‚   â”‚           â”œâ”€ SHAP: Feature contribution bar charts
â”‚   â”‚           â”œâ”€ Stats: Real-time KPIs & alerts
â”‚   â”‚           â””â”€ Refresh: 10s update interval
â”‚   â”‚
â”‚   â””â”€â”€ [Images auto-generated at runtime]
â”‚
â”œâ”€â”€ ğŸ§¬ mlflow/                                                â† Experiment Tracking Storage
â”‚   â”œâ”€â”€ mlflow.db                                             # SQLite database (created at runtime)
â”‚   â”‚   â”œâ”€ Stores: Experiment metadata, run info, parameters
â”‚   â”‚   â”œâ”€ Models: Registered model versions
â”‚   â”‚   â””â”€ Metrics: Training & validation metrics timeseries
â”‚   â”‚
â”‚   â””â”€â”€ artifacts/                                            # Model artifact storage
â”‚       â””â”€â”€ [Experiment folders created per run]
â”‚           â”œâ”€ Models: Trained .pkl files
â”‚           â”œâ”€ Metrics: JSON performance reports
â”‚           â””â”€ Params: Hyperparameter logs
â”‚
â”œâ”€â”€ ğŸ“‚ data/                                                  â† Data Storage (CSV files)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ system_metrics.csv                                # Original unprocessed metrics
â”‚   â”‚       â”œâ”€ Columns: timestamp, cpu, memory, network
â”‚   â”‚       â”œâ”€ Source: collect_real_data.py or data_generation.py
â”‚   â”‚       â””â”€ Format: Time-indexed CSV (rows = 24h samples)
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ system_metrics_processed.csv                      # Cleaned & standardized data
â”‚           â”œâ”€ Standardization: (x - mean) / std applied
â”‚           â”œâ”€ Missing values: Imputed or removed
â”‚           â”œâ”€ Duplicates: Removed
â”‚           â””â”€ Train/test split: 80/20 by default
â”‚
â”œâ”€â”€ ğŸ¤– models/                                                â† Trained Model Storage
â”‚   â”œâ”€â”€ anomaly_model_*.pkl                                   # Serialized Isolation Forest
â”‚   â”‚   â”œâ”€ Input: Features = [cpu, memory, network]
â”‚   â”‚   â”œâ”€ Output: anomaly_score âˆˆ [0, 1], is_anomaly âˆˆ {0, 1}
â”‚   â”‚   â”œâ”€ Loaded by: detect_anomaly.py (inference)
â”‚   â”‚   â””â”€ Versioning: Timestamped filenames
â”‚   â”‚
â”‚   â””â”€â”€ [Optional: Additional model variants]
â”‚
â”œâ”€â”€ ğŸ“ˆ results/                                               â† Outputs & Reports
â”‚   â”œâ”€â”€ training_metrics_*.json                               # Per-run training metrics
â”‚   â”‚   â”œâ”€ Metrics: precision, recall, f1, roc_auc, latency
â”‚   â”‚   â”œâ”€ Params: contamination, n_estimators, random_state
â”‚   â”‚   â””â”€ Timestamp: Run ID in filename
â”‚   â”‚
â”‚   â”œâ”€â”€ mlflow_metrics.json                                   # Consolidated MLflow metrics
â”‚   â”‚   â”œâ”€ Best model: Top hyperparameter configuration
â”‚   â”‚   â”œâ”€ Performance: Aggregated scores across grid search
â”‚   â”‚   â””â”€ Reproducibility: Git commit hash, Python version
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation_report_*.json                              # Robustness test results
â”‚   â”‚   â”œâ”€ Scenarios: Noise, missing data, outliers, shifts
â”‚   â”‚   â”œâ”€ Results: Per-scenario accuracy & latency
â”‚   â”‚   â””â”€ Recommendations: Model stability insights
â”‚   â”‚
â”‚   â””â”€â”€ detected_anomalies_for_testing.csv                    # Sample inference output
â”‚       â”œâ”€ Columns: timestamp, anomaly_score, is_anomaly, SHAP_*
â”‚       â””â”€ Example: Real predictions with feature contributions
â”‚
â”œâ”€â”€ ğŸ“œ requirements.txt                                        â† Python Dependencies
â”‚   â”œâ”€ Core ML: scikit-learn==1.8.0 (requires Python 3.11+)
â”‚   â”œâ”€ Explainability: SHAP>=0.42.0
â”‚   â”œâ”€ Databases: influxdb>=1.18.0, influxdb-client>=1.19.0
â”‚   â”œâ”€ Monitoring: MLflow>=2.0.0
â”‚   â”œâ”€ Data: pandas>=2.0.0, numpy>=1.23.0
â”‚   â”œâ”€ Testing: pytest>=7.0.0, pytest-cov>=4.0.0
â”‚   â””â”€ Others: python-dotenv, requests (for API calls)
â”‚
â”œâ”€â”€ ğŸ”§ setup_and_run.ps1                                      â† Automated Setup Script
â”‚   â”œâ”€ Checks: Docker installation & version
â”‚   â”œâ”€ Builds: Services if needed
â”‚   â”œâ”€ Starts: docker-compose up -d
â”‚   â”œâ”€ Waits: Service health checks (30s timeout per service)
â”‚   â”œâ”€ Initializes: Database, credentials, seeds data
â”‚   â””â”€ Launches: Browser tabs for Grafana, MLflow
â”‚
â”œâ”€â”€ ğŸ§ª test_and_validate.ps1                                  â† Testing Orchestration
â”‚   â”œâ”€ Runs: pytest with coverage
â”‚   â”œâ”€ Stress tests: HTTP load generation (configurable RPS)
â”‚   â”œâ”€ Anomaly injection: Triggers synthetic anomalies
â”‚   â”œâ”€ Reports: Coverage, latency, detection accuracy
â”‚   â””â”€ Output: Saves to results/ folder
â”‚
â”œâ”€â”€ â–¶ï¸  run.ps1                                                â† Quick Start Script
â”‚   â””â”€ Executes: docker-compose up -d (simplified)
â”‚
â”œâ”€â”€ ğŸ“– README.md                                              â† This file
â”œâ”€â”€ ğŸ³ docker command.txt                                     â† Manual docker-compose commands
â”‚   â”œâ”€ Alternative: If scripts fail, run commands directly
â”‚   â””â”€ Format: Copy-paste ready commands
â”‚
â”œâ”€â”€ âœ… check_datasource.py                                    â† InfluxDB Connectivity Check
â”‚   â”œâ”€ Verifies: InfluxDB connection & database
â”‚   â”œâ”€ Lists: Available measurements
â”‚   â””â”€ Debug: Helps troubleshoot connection issues
â”‚
â””â”€â”€ ğŸ” enable_datasource.py                                   â† Grafana Datasource Setup
    â”œâ”€ Creates: InfluxDB datasource in Grafana
    â”œâ”€ Auth: API calls to Grafana admin endpoint
    â””â”€ Idempotent: Safe to run multiple times
```

### Related Project Folders (Ecosystem)

These folders exist at the project root level and work with the core system:

#### ğŸ“¦ `../datacenter/` (Data Generation & Collection)
```
datacenter/
â”œâ”€â”€ flask_app/                       â† Metrics simulation server
â”‚   â”œâ”€â”€ app.py                       # Flask HTTP server (Port 5005)
â”‚   â”‚   â”œâ”€ Endpoint 1: /metrics â†’ Returns simulated system metrics
â”‚   â”‚   â”œâ”€ Endpoint 2: /stress â†’ Accepts stress test requests
â”‚   â”‚   â””â”€ Metrics: CPU%, Memory%, Network I/O
â”‚   â”‚
â”‚   â”œâ”€â”€ stress_test.py               # Stress testing helper
â”‚   â”œâ”€â”€ telegraf_flask.conf          # Telegraf input plugin for Flask
â”‚   â”œâ”€â”€ Dockerfile                   # Flask container definition
â”‚   â””â”€â”€ entrypoint.sh                # Startup script
â”‚
â”œâ”€â”€ docker-compose.yml               â† Datacenter service orchestration
â”‚   â”œâ”€ Service: Flask app (Port 5005)
â”‚   â””â”€ Service: Telegraf collector (pushes to InfluxDB)
â”‚
â”œâ”€â”€ telegraf.conf                    â† Telegraf system metrics collector
â”‚   â”œâ”€ CPU, Memory, Disk, Network collection
â”‚   â””â”€ Output: Pushes to InfluxDB (http://localhost:8086)
â”‚
â””â”€â”€ requirements.txt                 â† Flask dependencies

Purpose: Generates simulated & real infrastructure metrics
Data Flow: Flask/Telegraf â†’ InfluxDB (system_metrics) â†’ AI Pipeline
```

#### âš¡ `../stress-test-docker/` (Load Testing)
```
stress-test-docker/
â”œâ”€â”€ http_load_generator.py           â† HTTP request generator
â”‚   â”œâ”€ Threads: Configurable concurrent requests
â”‚   â”œâ”€ RPS: Adjustable requests-per-second
â”‚   â”œâ”€ Targets: Flask /metrics endpoint
â”‚   â””â”€ Metrics: Latency, throughput, success rate
â”‚
â”œâ”€â”€ stress.py                        â† Orchestration script
â”‚   â”œâ”€ Runs: Load generation with specified duration
â”‚   â”œâ”€ Reports: Latency percentiles (p50, p95, p99)
â”‚   â””â”€ Output: Stress test results JSON
â”‚
â”œâ”€â”€ docker-compose.yml               â† Stress test environment
â”‚   â”œâ”€ Service: HTTP load generator container
â”‚   â””â”€ Network: Links to Flask app
â”‚
â”œâ”€â”€ Dockerfile                       â† Load generator container
â”œâ”€â”€ entrypoint.sh                    â† Container startup
â””â”€â”€ requirements.txt                 â† Dependencies (requests, locust)

Purpose: Generates artificial system load for testing
Data Flow: Stress â†’ Flask â†’ InfluxDB â†’ AI Pipeline detects anomalies
```

### Architecture Data Flow Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Collection Phase                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Telegraf/Flask â†’ InfluxDB (system_metrics measurement)         â”‚
â”‚                 â””â”€ Stores: cpu, memory, network metrics         â”‚
â”‚                    Retention: Real-time + historical            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Training Phase                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ train_model.py:                                                 â”‚
â”‚   data/processed/system_metrics_processed.csv                  â”‚
â”‚     â†’ Isolation Forest (grid search)                            â”‚
â”‚     â†’ models/anomaly_model_*.pkl                                â”‚
â”‚     â†’ MLflow (experiment tracking)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Validation Phase (Continuous)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ validate_data.py:                                               â”‚
â”‚   InfluxDB system_metrics â†’ 6 validation checks â†’ Pass/Fail     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Real-Time Inference Phase                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ detect_anomaly.py (runs continuously):                          â”‚
â”‚   1. Read latest metrics from InfluxDB (system_metrics)        â”‚
â”‚   2. Load models/anomaly_model_*.pkl                           â”‚
â”‚   3. Predict: anomaly_score + is_anomaly                       â”‚
â”‚   4. Explain: SHAP feature contributions                       â”‚
â”‚   5. Write: InfluxDB ai_predictions measurement                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Visualization & Monitoring Phase                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Grafana Dashboard (localhost:3000):                             â”‚
â”‚   - Real-time anomaly heatmap & alerts                         â”‚
â”‚   - SHAP feature contribution panels                           â”‚
â”‚   - System metrics time series                                 â”‚
â”‚                                                                 â”‚
â”‚ MLflow UI (localhost:5000):                                     â”‚
â”‚   - Experiment history & metrics                               â”‚
â”‚   - Model artifacts & parameters                               â”‚
â”‚   - Run comparison & best model tracking                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Service Dependencies

```
docker-compose.yml orchestration:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ai_app (Python 3.11)             â”‚  â† Main ML service
â”‚  depends_on: influxdb (healthy)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ reads/writes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      influxdb:1.8.10 (Port 8086)         â”‚  â† Time-series storage
â”‚  health_check: /_health endpoint         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ reads data
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        grafana:latest (Port 3000)        â”‚  â† Visualization
â”‚  auto_provisioned: InfluxDB datasource   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optional:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       mlflow (Port 5000)                 â”‚  â† Experiment tracking
â”‚  backend: mlflow/mlflow.db (SQLite)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
â”œâ”€â”€ models/                          # Trained models with versioning
â”‚   â””â”€â”€ anomaly_model_v20260128_145609.pkl
â”œâ”€â”€ results/                         # Training/evaluation reports
â”‚   â”œâ”€â”€ training_metrics_*.json
â”‚   â”œâ”€â”€ validation_report_*.json
â”‚   â””â”€â”€ evaluation_report_*.json
â”œâ”€â”€ docs/                            # Comprehensive documentation
â”‚   â”œâ”€â”€ REQUIREMENTS.md              # Problem statement & KPIs
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # System design & data flow
â”‚   â”œâ”€â”€ MODEL_CARD.md                # Algorithm & robustness
â”‚   â””â”€â”€ DEPLOYMENT.md                # Operations guide
â”œâ”€â”€ .github/workflows/tests.yml      # CI/CD pipeline
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

### Related Folders in the Workspace

- [../datacenter](../datacenter) â€” Flask-based metric generator + Telegraf config used for realistic data ingestion.
    - docker-compose.yml, telegraf.conf
    - flask_app/ (app.py, stress_test.py, Dockerfile, entrypoint.sh)
    - flask_logs/ (runtime logs)
- [../stress-test-docker](../stress-test-docker) â€” Standalone HTTP load generator for stress testing.
    - docker-compose.yml, Dockerfile, entrypoint.sh
    - http_load_generator.py, stress.py, requirements.txt

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# Log level (INFO, DEBUG, WARNING, ERROR)
export LOG_LEVEL=INFO

# MLflow tracking URI
export MLFLOW_TRACKING_URI=http://mlflow:5000

# InfluxDB connection
export INFLUXDB_HOST=influxdb
export INFLUXDB_PORT=8086
export INFLUXDB_DATABASE=system_metrics
```

### Docker Customization

Edit `docker/docker-compose.yml` to adjust:
- Port numbers (3000, 5000, 8086)
- Resource limits (CPU, RAM)
- Healthcheck intervals
- Environment variables

---

## ğŸ“Š Dashboard Access

### Grafana (http://localhost:3000)
- **Login**: admin / admin
- **Dashboard**: "AI Anomaly Detection" (auto-loads)
- **Features**: Real-time anomalies, trends, alerts, custom panels

### MLflow (http://localhost:5000)
- **Experiments**: anomaly_detection_training
- **Metrics**: Precision, Recall, F1, ROC-AUC
- **Artifacts**: Models, scalers, reports
- **Comparison**: A/B test runs

---

## ğŸ” Monitoring & Logging

### View Logs

```bash
# All services with timestamps
docker-compose -f docker/docker-compose.yml logs -f --timestamps

# Specific service
docker-compose -f docker/docker-compose.yml logs -f ai_app

# Real-time with filtering
docker-compose -f docker/docker-compose.yml logs -f ai_app | grep "ERROR"
```

### Structured Logging

All scripts use Python `logging` module:

```bash
# View with different verbosity
export LOG_LEVEL=DEBUG  # Verbose
export LOG_LEVEL=INFO   # Normal
export LOG_LEVEL=ERROR  # Production
```

### Health Checks

```bash
# View service health status
docker-compose -f docker/docker-compose.yml ps

# Detailed health info
docker inspect docker-ai_app-1 | grep -A 5 Health
```

---

## ğŸš€ Common Tasks

### Automated Workflows

```powershell
# Complete setup from scratch
.\setup_and_run.ps1 -CleanInstall

# Run validation tests
.\test_and_validate.ps1

# Custom stress test (5 min, 500 RPS)
.\test_and_validate.ps1 -Duration 300 -RequestsPerSecond 500
```

### Manual Operations

```bash
# Train a new model
docker-compose -f docker/docker-compose.yml exec -T ai_app python src/train_model.py

# Collect real data from InfluxDB
docker-compose -f docker/docker-compose.yml exec -T ai_app python src/collect_real_data.py

# Validate data quality
docker-compose -f docker/docker-compose.yml exec -T ai_app python src/validate_data.py

# Test model robustness
docker-compose -f docker/docker-compose.yml exec -T ai_app python src/evaluate_model.py

# Run unit tests
docker-compose -f docker/docker-compose.yml exec -T ai_app pytest tests/ -v
```

---

## ğŸ“š Documentation

Full documentation in `/docs`:

| Document | Content |
|----------|---------|
| [AUTOMATION_GUIDE.md](docs/AUTOMATION_GUIDE.md) | Automated setup & testing scripts (NEW) |
- [TIER_1_SUMMARY.md](../TIER_1_SUMMARY.md) - MLOps core (8 components)
- [TIER_2_SUMMARY.md](../TIER_2_SUMMARY.md) - QA & operations (6 components)
- [IMPLEMENTATION_ROADMAP.md](../IMPLEMENTATION_ROADMAP.md) - Course alignment

---

## ğŸ”„ MLOps Pipeline

### Training Workflow
1. Load historical metrics (CSV)
2. Validate data quality (6 checks)
3. Split 70/15/15 (train/val/test)
4. Grid search hyperparameters
5. Train Isolation Forest
6. Evaluate metrics
7. Version model with timestamp
8. Log to MLflow

### Inference Workflow
1. Query live metrics from InfluxDB
2. Apply fitted scaler
3. Get predictions + anomaly scores
4. Write results to InfluxDB
5. View SHAP feature contributions
6. Trigger retraining if needed

---

## ğŸ› Troubleshooting

### Services Not Starting
```bash
# Check Docker daemon
docker --version

# View detailed logs
docker-compose -f docker/docker-compose.yml logs

# Rebuild fresh
docker-compose -f docker/docker-compose.yml down -v
docker-compose -f docker/docker-compose.yml up -d --build
```

### Grafana Dashboard Not Loading
```bash
# Wait 30 seconds for startup
# Refresh browser
# Check provisioning:
docker exec docker-grafana-1 ls /etc/grafana/provisioning/dashboards/
```

### Tests Failing
```bash
# Verbose output
docker-compose -f docker/docker-compose.yml exec -T ai_app pytest tests/ -vv

# Verify data exists
docker-compose -f docker/docker-compose.yml exec -T ai_app ls -la data/processed/
```

---

## ğŸ“ Course Information

**Course**: AI Systems Engineering  
**Semester**: Fall 2025  
**Project Type**: Innovation-driven (INN)  

**Requirements Coverage**:
- âœ… Part I: Design (REQUIREMENTS.md, ARCHITECTURE.md)
- âœ… Part II: Development (train_model.py, validate_data.py)
- âœ… Part III: Verification (evaluate_model.py, tests/)
- âœ… Part IV: Operations (docker-compose.yml, grafana/, mlflow/)

---

## ğŸ“Š Project Statistics

- **Python Code**: 1000+ lines
- **Tests**: 5 unit tests (100% passing)
- **Documentation**: 65+ pages
- **Docker Services**: 4 (with health checks)
- **CI/CD**: GitHub Actions workflow
- **Git Commits**: 15+ with detailed history

---

## ğŸ” Security & License

- **License**: MIT
- **Security**: Dependency scanning via GitHub Actions
- **Code Quality**: Linting and vulnerability checks

---

## ğŸ¯ Implemented Tier 3 Features

**Explainability & Monitoring** (Fully Implemented):
- âœ… **SHAP Values Integration**: Per-feature anomaly contribution analysis (detect_anomaly.py)
- âœ… **Grafana Dashboards**: Real-time visualization + SHAP panels
- âœ… **Secrets Management**: Environment variables for secure credential handling
- âœ… **Complete Monitoring Stack**: InfluxDB + Grafana + MLflow

**Not Implemented** (Out of Scope):
- âŒ MLflow Model Registry (versioning)
- âŒ Blue-green deployment strategy
- âŒ Kubernetes deployment manifests
- âŒ Advanced drift detection (5 statistical tests)

---

**Repository**: https://github.com/Shehpar/ai-system-engineering  
**Last Updated**: January 28, 2026  
**Status**: âœ… Production Ready
